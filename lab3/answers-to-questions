2.1(a) State space consists of all possible (x, y, heading) configurations of the robot in the grid space. The cardinality of the state space is 12|L||W| = 432 for the case where L = W = 6.

2.1(b) Actions include {stay still} + {forwards, backwards} x {rotate clockwise, rotate counterclockwise, no rotation}. Thus, the cardinality of the action space is 7.

2.1(c) function implemented in get_p() method in the Environment class in file env.py.

2.1(d) function implemented in get_next_state() method in the Environment class in file env.py.

2.2(a) The reward can be found using the get_reward_at() function in the Environment class

2.3(a) The initial policy is generated using get_init_policy() and action_to_take()

2.3(b) The trajectory is generated in the main loop in mdp_system.py and plotted using the Visualizer class

2.3(c) See report for trajectory.

2.3(d) policy_eval() in env.py computes policy evaluation

2.3(e) The trajectory found in 2.3(c) had a value of roughly -3

2.3(f) one_step_look() inside policy_iteration() computes the one step lookahead

2.3(g) policy_iteration() in env.py performs the policy iteration algorithm

2.3(h) The trajectory found in value of roughly 10.722

2.3(i) It took approximately 400 seconds to complete the policy iteration

2.4(a) Function implemented in value_iteration():

      	Value Iteration Algorithm.

        Args:
            state: Starting state of the robot (State, nS)
            theta: We stop evaluation once our value function change is less than theta for all states. 
            gamma: discount factor
        Returns:
            A tuple (policy, V) of the optimal policy and the optimal value function.
                Policy: 3 dimensional array of items of Action class, nS
                V: vector of nS.

       It uses a helper function:

            Helper function to calculate the value for all action in a given state.
            
            Args:
                state: The state to consider (State)
                V: The value to use as an estimator, Vector of length nS
            
            Returns:
                A vector of length nA containing the expected value of each action.

2.4(b) The function finds same trajectory under p_e = 0 and slight variations occur when p_e is nonzero. 

2.4(c) For example: With Gamma 0.9, p_e = 0, it takes 2mins. This result is significantly (one third) faster than policy iteration with same conditions.

2.5(a) 	PolicyIter: with p_e = 10%, the algorithm takes roughly 910 seconds
	ValueIter: With p_e non zero, it takes almost 3 mins. Still significantly faster than Policy Iteration.
2.5(b)

2.5(c) The policy iteration algorithm generally took much longer than the value iteration algorithm. Additionally,
       A prerotation chance greatly increased the time to convergence for both algorithms.
